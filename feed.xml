<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://changelog.hpc.shef.ac.uk//feed.xml" rel="self" type="application/atom+xml" /><link href="http://changelog.hpc.shef.ac.uk//" rel="alternate" type="text/html" /><updated>2023-04-19T12:31:34+00:00</updated><id>http://changelog.hpc.shef.ac.uk//feed.xml</id><title type="html">TUoS HPC Changelog</title><subtitle>Keep yourself up to date with recent changes on the University of Sheffield HPC clusters.</subtitle><entry><title type="html">Major HPC documentation update: New Stanage cluster launched!</title><link href="http://changelog.hpc.shef.ac.uk//stanage-launch/" rel="alternate" type="text/html" title="Major HPC documentation update: New Stanage cluster launched!" /><published>2023-04-19T00:00:00+00:00</published><updated>2023-04-19T00:00:00+00:00</updated><id>http://changelog.hpc.shef.ac.uk//stanage-launch</id><content type="html" xml:base="http://changelog.hpc.shef.ac.uk//stanage-launch/"><![CDATA[<p>A major update to the HPC documentation has been made to align with the launch of our new Stanage cluster</p>

<p>The new documentation can be found at:</p>

<p><a href="https://docs.hpc.shef.ac.uk/en/latest/stanage/index.html">https://docs.hpc.shef.ac.uk/en/latest/stanage/index.html</a></p>]]></content><author><name></name></author><category term="New" /><category term="documentation" /><category term="update" /><category term="Stanage" /><summary type="html"><![CDATA[A major update to the HPC documentation has been made to align with the launch of our new Stanage cluster]]></summary></entry><entry><title type="html">Cluster Git modules updated - new version: 2.39.1</title><link href="http://changelog.hpc.shef.ac.uk//software-git-upgrades/" rel="alternate" type="text/html" title="Cluster Git modules updated - new version: 2.39.1" /><published>2023-01-30T00:00:00+00:00</published><updated>2023-01-30T00:00:00+00:00</updated><id>http://changelog.hpc.shef.ac.uk//software-git-upgrades</id><content type="html" xml:base="http://changelog.hpc.shef.ac.uk//software-git-upgrades/"><![CDATA[<p>Newer Git modules for both the ShARC and Bessemer cluster have been added and older modules have been depreciated due to various vulnerabilities.</p>

<p>Git can now be loaded on ShARC with:</p>

<blockquote>
  <p><code class="language-plaintext highlighter-rouge">module load dev/git/2.39.1/gcc-4.9.4</code></p>
</blockquote>

<p>Git can now be loaded on Bessemer with:</p>

<blockquote>
  <p><code class="language-plaintext highlighter-rouge">module load git/2.39.1-GCCcore-10.3.0-nodocs</code>.</p>
</blockquote>]]></content><author><name></name></author><category term="New" /><category term="Bessemer" /><category term="ShARC" /><category term="HPC" /><category term="maintenance" /><category term="Git" /><summary type="html"><![CDATA[Newer Git modules for both the ShARC and Bessemer cluster have been added and older modules have been depreciated due to various vulnerabilities.]]></summary></entry><entry><title type="html">Bessemer NVIDIA A100 GPU Nodes removed.</title><link href="http://changelog.hpc.shef.ac.uk//bessemer-gpu-nodes-removed/" rel="alternate" type="text/html" title="Bessemer NVIDIA A100 GPU Nodes removed." /><published>2022-12-21T00:00:00+00:00</published><updated>2022-12-21T00:00:00+00:00</updated><id>http://changelog.hpc.shef.ac.uk//bessemer-gpu-nodes-removed</id><content type="html" xml:base="http://changelog.hpc.shef.ac.uk//bessemer-gpu-nodes-removed/"><![CDATA[<p>Between May and December 2022, 16 addtional GPU nodes were (temporarily) available to all users of Bessemer.<br />
These featured NVIDIA A100 GPUs, which were quite a bit faster than the (older generation) V100 GPUs in Bessemer.</p>

<p>These have now been removed from Bessemer and will be available in the University’s new HPC cluster, Stanage, early in 2023.</p>]]></content><author><name></name></author><category term="New" /><category term="Bessemer" /><category term="HPC" /><category term="maintenance" /><category term="GPUs" /><category term="GPU" /><category term="A100" /><summary type="html"><![CDATA[Between May and December 2022, 16 addtional GPU nodes were (temporarily) available to all users of Bessemer. These featured NVIDIA A100 GPUs, which were quite a bit faster than the (older generation) V100 GPUs in Bessemer.]]></summary></entry><entry><title type="html">Bessemer scheduler update from version 22.05.5 to 22.05.6</title><link href="http://changelog.hpc.shef.ac.uk//bessemer-slurm-maintenance-1/" rel="alternate" type="text/html" title="Bessemer scheduler update from version 22.05.5 to 22.05.6" /><published>2022-11-14T00:00:00+00:00</published><updated>2022-11-14T00:00:00+00:00</updated><id>http://changelog.hpc.shef.ac.uk//bessemer-slurm-maintenance-1</id><content type="html" xml:base="http://changelog.hpc.shef.ac.uk//bessemer-slurm-maintenance-1/"><![CDATA[<p>An update is required for the Slurm scheduler  (from version 22.05.5 to 22.05.6) on Bessemer. This maintenance is scheduled for Monday the 14th of November between 11:00 to 12:00.</p>

<p>This should be non-disruptive but if you wish, you can hold and release your queued jobs with:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>scontrol hold &lt;job ID&gt; 

scontrol release &lt;job ID&gt;
</code></pre></div></div>

<p>If you have any questions about the above or the maintenance work in general please get in touch with us via <a href="mailto:research-it@sheffield.ac.uk?subject=RE:Bessemer Slurm update November 14th (maintenance work)">research-it@sheffield.ac.uk</a>.</p>]]></content><author><name></name></author><category term="New" /><category term="Bessemer" /><category term="HPC" /><category term="maintenance" /><category term="Slurm" /><summary type="html"><![CDATA[An update is required for the Slurm scheduler (from version 22.05.5 to 22.05.6) on Bessemer. This maintenance is scheduled for Monday the 14th of November between 11:00 to 12:00.]]></summary></entry><entry><title type="html">Bessemer scheduler update from version 21.08.8 to 22.05.5</title><link href="http://changelog.hpc.shef.ac.uk//bessemer-slurm-maintenance/" rel="alternate" type="text/html" title="Bessemer scheduler update from version 21.08.8 to 22.05.5" /><published>2022-11-01T00:00:00+00:00</published><updated>2022-11-01T00:00:00+00:00</updated><id>http://changelog.hpc.shef.ac.uk//bessemer-slurm-maintenance</id><content type="html" xml:base="http://changelog.hpc.shef.ac.uk//bessemer-slurm-maintenance/"><![CDATA[<p>An update is required for the Slurm scheduler  (from version 21.08.8 to 22.05.5)  on Bessemer. This maintenance is scheduled for Thursday the 3rd of November between 13:00 to 14:00.</p>

<p>This should be non-disruptive but if you wish, you can hold and release your queued jobs with:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>scontrol hold &lt;job ID&gt; 

scontrol release &lt;job ID&gt;
</code></pre></div></div>

<p>If you have any questions about the above or the maintenance work in general please get in touch with us via <a href="mailto:research-it@sheffield.ac.uk?subject=RE:Bessemer Slurm update November 3rd (maintenance work)">research-it@sheffield.ac.uk</a>.</p>]]></content><author><name></name></author><category term="New" /><category term="Bessemer" /><category term="HPC" /><category term="maintenance" /><category term="Slurm" /><summary type="html"><![CDATA[An update is required for the Slurm scheduler (from version 21.08.8 to 22.05.5) on Bessemer. This maintenance is scheduled for Thursday the 3rd of November between 13:00 to 14:00.]]></summary></entry><entry><title type="html">Bessemer fastdata filestore offline on the 1st of September (maintenance work)</title><link href="http://changelog.hpc.shef.ac.uk//bessemer-filestore-maintenance-1/" rel="alternate" type="text/html" title="Bessemer fastdata filestore offline on the 1st of September (maintenance work)" /><published>2022-09-01T00:00:00+00:00</published><updated>2022-09-01T00:00:00+00:00</updated><id>http://changelog.hpc.shef.ac.uk//bessemer-filestore-maintenance-1</id><content type="html" xml:base="http://changelog.hpc.shef.ac.uk//bessemer-filestore-maintenance-1/"><![CDATA[<p>An update is required for the storage controllers, Lustre clients and Lustre servers underpinning of the fastdata filestorage area on the Bessemer HPC system. The /fastdata area (Lustre filesystem) on Bessemer will be unavailable on Thursday the 1st of September from 09:00 to 17:00.</p>

<p>Please be aware that:</p>
<ul>
  <li>Any jobs using /fastdata at 09:00 on the 1st of September will be terminated.</li>
  <li>Any processes on login nodes using /fastdata at that time will also be terminated.</li>
  <li>Bessemer will be accessible and usable throughout this maintenance period for those that don’t need access to /fastdata then.</li>
</ul>

<p>If you have any questions about the above or the maintenance work in general please get in touch with us via <a href="mailto:research-it@sheffield.ac.uk?subject=RE:Bessemer fastdata file store at risk on the September 1st (maintenance work)">research-it@sheffield.ac.uk</a>.</p>]]></content><author><name></name></author><category term="New" /><category term="Bessemer" /><category term="HPC" /><category term="maintenance" /><summary type="html"><![CDATA[An update is required for the storage controllers, Lustre clients and Lustre servers underpinning of the fastdata filestorage area on the Bessemer HPC system. The /fastdata area (Lustre filesystem) on Bessemer will be unavailable on Thursday the 1st of September from 09:00 to 17:00.]]></summary></entry><entry><title type="html">NAG C and Fortran library version 28.5 (nll6i285bl) installed on ShARC</title><link href="http://changelog.hpc.shef.ac.uk//NAG-lib-update-sharc/" rel="alternate" type="text/html" title="NAG C and Fortran library version 28.5 (nll6i285bl) installed on ShARC" /><published>2022-08-22T00:00:00+00:00</published><updated>2022-08-22T00:00:00+00:00</updated><id>http://changelog.hpc.shef.ac.uk//NAG-lib-update-sharc</id><content type="html" xml:base="http://changelog.hpc.shef.ac.uk//NAG-lib-update-sharc/"><![CDATA[<p>Version 28.5 of the NAG C and Fortran library has been installed on ShARC and version 27 has been deprecated as it’s no longer supported or licensed.</p>

<p><a href="https://docs.hpc.shef.ac.uk/en/latest/sharc/software/libs/nag-libs.html">More information</a>.</p>]]></content><author><name></name></author><category term="New" /><category term="NAG" /><category term="software" /><category term="ShARC" /><summary type="html"><![CDATA[Version 28.5 of the NAG C and Fortran library has been installed on ShARC and version 27 has been deprecated as it’s no longer supported or licensed.]]></summary></entry><entry><title type="html">Bessemer fastdata filestore at risk on the July 27th (maintenance work)</title><link href="http://changelog.hpc.shef.ac.uk//bessemer-filestore-maintenance/" rel="alternate" type="text/html" title="Bessemer fastdata filestore at risk on the July 27th (maintenance work)" /><published>2022-07-12T00:00:00+00:00</published><updated>2022-07-12T00:00:00+00:00</updated><id>http://changelog.hpc.shef.ac.uk//bessemer-filestore-maintenance</id><content type="html" xml:base="http://changelog.hpc.shef.ac.uk//bessemer-filestore-maintenance/"><![CDATA[<p>An update is required for the storage controllers of the fastdata filestorage area on the Bessemer HPC system. No downtime is required or expected however <strong>the fastdata area on Bessemer should be considered at risk during this maintenance.</strong></p>

<p>The maintenance period for this work will be 11:00 - 13:00 on the 27th of July. Further reminders and updates with respect to this maintenance will be provided in due course.</p>

<p>If you have any questions about the above or the maintenance work in general please get in touch with us via <a href="mailto:research-it@sheffield.ac.uk?subject=RE:Bessemer fastdata file store at risk on the July 27th (maintenance work)">research-it@sheffield.ac.uk</a>.</p>

<p>Note: This work was rescheduled due to the abnormally hot weather from the previous week causing a major incident via loss of air conditioning in the datacentre.</p>]]></content><author><name></name></author><category term="New" /><category term="Bessemer" /><category term="HPC" /><category term="maintenance" /><summary type="html"><![CDATA[An update is required for the storage controllers of the fastdata filestorage area on the Bessemer HPC system. No downtime is required or expected however the fastdata area on Bessemer should be considered at risk during this maintenance.]]></summary></entry><entry><title type="html">ShARC unavailable on 5th July (filestore maintenance work)</title><link href="http://changelog.hpc.shef.ac.uk//sharc-filestore-maintenance/" rel="alternate" type="text/html" title="ShARC unavailable on 5th July (filestore maintenance work)" /><published>2022-06-20T00:00:00+00:00</published><updated>2022-06-20T00:00:00+00:00</updated><id>http://changelog.hpc.shef.ac.uk//sharc-filestore-maintenance</id><content type="html" xml:base="http://changelog.hpc.shef.ac.uk//sharc-filestore-maintenance/"><![CDATA[<p>On 5th July ShARC will be unavailable due to maintenance work, as data in several key storage areas including home directories is being migrated from older file servers to newer ones.  Access to and use of the Bessemer HPC cluster will not be affected by this work, neither will direct user access to research storage areas via CIFS/SMB clients.</p>

<p>The data migration work is scheduled to take place between 09:00 to 12:00 on that day but all user activity on the cluster should cease before then and you should assume that the cluster won’t be usable earlier in the day on 5th July.  We’ll update users via hpc-announce@sheffield.ac.uk once the work is complete and should there be any delays.</p>

<p>Any user activity on the cluster at 09:00 of 5th July will be terminated including: SSH sessions on login nodes, interactive and batch jobs on worker nodes, interactive JupyterHub and myApps/SGD sessions, periodic jobs and file transfers to/from the cluster (via SCP/rsync).  During the maintenance period users will not be able to SSH to the login nodes, copy files to/from the cluster via SCP/rsync, run JupyterHub sessions or run myApps sessions.</p>

<p>Job queues will be automatically disabled to prevent the submission of new jobs after 00:00 on 5th July.  Any jobs submitted in the few days before that will remain queued and won’t run until after the maintenance period if the job scheduler (SGE) thinks the jobs require more run time than is available before 00:00 on 5th July.  Don’t forget that you can explicitly specify the amount of run time you want per job on ShARC using the <a href="https://docs.hpc.shef.ac.uk/en/latest/hpc/scheduler/index.html#batch-jobs">SGE parameter ‘-l h_rt’</a></p>

<p>If you have any questions about the above or the maintenance work in general please get in touch with us via <a href="mailto:research-it@sheffield.ac.uk?subject=RE:ShARC unavailable on 5th July (maintenance work)">research-it@sheffield.ac.uk</a>.</p>]]></content><author><name></name></author><category term="New" /><category term="ShARC" /><category term="HPC" /><category term="maintenance" /><summary type="html"><![CDATA[On 5th July ShARC will be unavailable due to maintenance work, as data in several key storage areas including home directories is being migrated from older file servers to newer ones. Access to and use of the Bessemer HPC cluster will not be affected by this work, neither will direct user access to research storage areas via CIFS/SMB clients.]]></summary></entry><entry><title type="html">64 NVIDIA A100 GPUs are now temporarily available on Bessemer for general use</title><link href="http://changelog.hpc.shef.ac.uk//bessemer-a100-nodes/" rel="alternate" type="text/html" title="64 NVIDIA A100 GPUs are now temporarily available on Bessemer for general use" /><published>2022-05-23T00:00:00+00:00</published><updated>2022-05-23T00:00:00+00:00</updated><id>http://changelog.hpc.shef.ac.uk//bessemer-a100-nodes</id><content type="html" xml:base="http://changelog.hpc.shef.ac.uk//bessemer-a100-nodes/"><![CDATA[<p>Those of you that have a need for GPUs for your work may be aware that the Bessemer cluster only has one public GPU node (with 4x NVIDIA V100 GPUs) and ShARC only has two public GPU nodes (with K80 GPUs, which are now not particularly modern). There are a number of private GPUs node in Bessemer and ShARC and the private GPU nodes in Bessemer can be used by all users if they’re willing to tolerate <a href="https://docs.hpc.shef.ac.uk/en/latest/hpc/scheduler/index.html#preemptable-jobs">jobs being abruptly terminated</a>, but this isn’t a constraint all researchers can work with and having more GPUs available to the UoS research community would be a good thing.</p>

<p>We’re therefore pleased to announce that we’re to greatly increase the number of GPUs available in UoS HPC systems by temporarily adding 16 GPU nodes, each with 4x NVIDIA A100 GPUs, to the Bessemer HPC system; and these will be available to all users. Note that these nodes will be relocated into what will be our next big HPC system in a few months’ time (there’ll be more info on that HPC system in due course).</p>

<p>Peter Heywood and Robert Chisholm in the Research Software Engineering team have done some performance comparisons with the <a href="https://flamegpu.com/">FLAME GPU 2</a> research software between these A100 nodes and the existing V100 nodes in Bessemer. They found that even for a memory-latency (and memory-bandwidth) -bound problem that the nodes featuring newer-generation A100 GPUs were significantly faster than the V100 nodes under CUDA 11!</p>

<p>Please note:</p>
<ul>
  <li>These nodes are being provided in Bessemer on a temporary basis.</li>
  <li>They can be used for interactive work and for batch jobs (but please submit batch jobs where possible)</li>
  <li>Much of the software centrally provided on Bessemer is incompatible with these nodes as they have a different CPU ‘microarchitecture’ to the majority of nodes in Bessemer.</li>
  <li>Some software is centrally-provided specifically for use on these nodes.</li>
  <li>But it’s currently quite a limited set and will most likely not be expanded on much until the nodes are relocated into our next big HPC system in a few months’ time.</li>
  <li>There are a few things to be aware of if migrating workflows to these nodes, which we’ve tried to document.</li>
</ul>

<p>For more information on these nodes including specifications, access information and warnings/gotchas regarding software performance and compatibility please see:</p>

<p><a href="https://docs.hpc.shef.ac.uk/en/latest/bessemer/GPUComputingBessemer.html">Using GPUs on Bessemer — Sheffield HPC Documentation</a></p>

<p>If you have any comments/feedback regarding these nodes or the documentation, or have any issues using them then please get in touch via <a href="mailto:research-it@sheffield.ac.uk?subject=A100 usage help request.&quot;">research-it@sheffield.ac.uk</a>.</p>]]></content><author><name></name></author><category term="New" /><category term="Bessemer" /><category term="HPC" /><category term="GPUs" /><category term="A100" /><category term="NVIDIA" /><summary type="html"><![CDATA[Those of you that have a need for GPUs for your work may be aware that the Bessemer cluster only has one public GPU node (with 4x NVIDIA V100 GPUs) and ShARC only has two public GPU nodes (with K80 GPUs, which are now not particularly modern). There are a number of private GPUs node in Bessemer and ShARC and the private GPU nodes in Bessemer can be used by all users if they’re willing to tolerate jobs being abruptly terminated, but this isn’t a constraint all researchers can work with and having more GPUs available to the UoS research community would be a good thing.]]></summary></entry></feed>